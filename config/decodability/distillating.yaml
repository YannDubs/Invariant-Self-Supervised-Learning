defaults:
  - base_distillating

kwargs:
  mode: assign_self_distillation
  beta_pM_unif: 1.9
  beta_HMlZ: 1.5 # cannot be less than one
  ema_weight_prior: 0.7
  out_dim: 16384
  temperature: 1
  temperature_assign: null # by default half of temperature
  is_batchnorm_pre: False
  is_reweight_ema: True
  projector_kwargs:
    architecture: "mlp"
    bottleneck_size: 512 # bottleneck for the last layer, ie, low rank approx
  predictor_kwargs:
    architecture: "linear"
    bottleneck_size: 512 # bottleneck for the last layer, ie, low rank approx