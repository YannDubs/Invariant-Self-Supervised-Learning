defaults:
  - base_distillating

kwargs:
  mode: assign_self_distillation
  beta_pM_unif: 1.9
  beta_HMlZ: 1.5 # cannot be less than one
  ema_weight_prior: 0.7
  out_dim: 16384
  temperature: 1
  temperature_assign: null # by default half of temperature
  is_batchnorm_pre: False
  is_reweight_ema: True
  batchnorm_kwargs: {}
  projector_kwargs:
    architecture: "mlp"
    bottleneck_size: ${decodability.kwargs.predictor_kwargs.bottleneck_size}
    is_batchnorm_bottleneck: ${decodability.kwargs.predictor_kwargs.is_batchnorm_bottleneck}
    batchnorm_kwargs: ${decodability.kwargs.predictor_kwargs.batchnorm_kwargs}
    is_train_bottleneck: ${decodability.kwargs.predictor_kwargs.is_train_bottleneck}
  predictor_kwargs:
    architecture: "linear"
    bottleneck_size: 512 # bottleneck for the last layer, ie, low rank approx
    is_batchnorm_bottleneck: False # can improve a little
    is_train_bottleneck: True
    batchnorm_kwargs:
      affine: False # don't train as will be sandwiched around linear layers => useless param