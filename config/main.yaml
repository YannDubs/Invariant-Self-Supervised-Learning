defaults:

  # GENERAL #
  - _self_
  - user

  - logger: wandb
  - server: none
  - hypopt: none
  - mode: none

  # REPRESENTOR #
  - data@data_repr: mnist
  - architecture@encoder: resnet18
  - architecture@online_evaluator: linear
  - checkpoint@checkpoint_repr: bestTrainLoss
  - optimizer@optimizer_issl: Adam_lr3e-3_w0 # larger learning rate will decrease I[Z,X] usually, so can be seen as regularizer
  - scheduler@scheduler_issl: warm_unifmultistep1000
  - optimizer@optimizer_eval: Adam_lr3e-4_w0
  - scheduler@scheduler_eval: unifmultistep100 # ensure that can always keep up with the changing representation
  - decodability: contrastive
  - regularizer: none
  - finetune: none # change if using pretrained representor.
  - representor: base

  # downstream task #
  - checkpoint@checkpoint_pred: bestTrainLoss # by default no validation set for predictors
  - optimizer@optimizer_pred: Adam_lr3e-4_w0
  - scheduler@scheduler_pred: unifmultistep100

  # OVERRIDES #
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


########## GENERAL ##########
experiment: ???
job_id: ??? # unique identifier
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because time is not when job runs
long_name_repr: exp_${experiment}/datarepr_${data_repr.name}/augrepr_${list2str:${data_repr.kwargs.dataset_kwargs.a_augmentations}}/repr_${representor.name}/dec_${decodability.name}/enc_${encoder.name}/reg_${regularizer.name}/optrepr_${optimizer_issl.name}/schedrepr_${scheduler_issl.name}/zdim_${encoder.z_shape}/zs_1/beta_${format:${representor.loss.beta},.1e}/seed_${seed}/addrepr_${other.add_repr}
long_name_pred: ${long_name_repr}/datapred_${data_pred.name}/pred_${predictor.name}/optpred_${optimizer_pred.name}/schedpred_${scheduler_pred.name}/addpred_${other.add_pred}
is_return: false # will return everything (the data, the models, ...)
monitor_return: [] # normal return of the function. This is especially useful for hyperparameter tuning
monitor_direction: [] # whether the monitor should be maximized or minimized. useful for hyperparameter tuning
is_no_save: ${is_return} # if you are sure you don't want to save anything
is_skip_pred: false # run predictor?
is_nlp_cluster: False # should the working directory be used as main

paths: #! the best practice is not to modify those paths but to symlink them to the places you want
  relative_work: outputs/${now:%Y-%m-%d_%H-%M-%S}
  relative_checkpoint: checkpoints/${long_name}/jid_${job_id}
  work: ${hydra.runtime.cwd}/${paths.relative_work} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  base_dir: ${hydra:runtime.cwd} # actual script where you are running from and want to save stuff
  tmp_dir: ${paths.base_dir} # main directory for all things that are only used when running script

  data: ${paths.tmp_dir}/data
  logs: ${paths.tmp_dir}/logs/${long_name}/jid_${job_id}
  checkpoint: ${paths.tmp_dir}/${paths.relative_checkpoint} # checkpoint to use during training

  exit_checkpoint: ${paths.base_dir}/${paths.relative_checkpoint} # checkpoint to use in case you get preempted
  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  pretrained:
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/*  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest

other: # some meta information that can be useful for internal stuff (usually dirty workarounds or for logging)
  is_debug: False # using debug mode
  is_quick: False # using a "quick" mode and should not log very slow things
  hydra_job_id: ${hydra:job.id} # this is the job id without the sweep number. Useful for filtering and grouping in wandb
  add_repr: null # some additional value for saving (e.g. current sweeping values)
  add_pred: null # some additional value for saving (e.g. current sweeping values)
  git_hash: null

### STAGE SPECIFIC ###
stage: ???
long_name: ???
checkpoint: {}
data: {}

### RUNNING ###
evaluation:
  is_eval_on_test: True # whether to evaluate on test. If not uses validation which is necessary if you don't have access to test set
  representor:
    ckpt_path: "best"
    is_evaluate: ${representor.is_train}
    is_online_eval: True
    is_eval_train: False # whether to evaluate the representor on train. Can be slow
  predictor:
    ckpt_path: "best"
    is_evaluate: ${predictor.is_train}
    is_eval_train: True # whether to evaluate the predictor on train. Can be slow

callbacks: # can use any callback name of issl.callbacks, pl.callbacks
  is_force_no_additional_callback: false # force empty callback

  # all callback kwargs should be here (`is_use` just says whether to use that callback)
  LearningRateMonitor:
    is_use : true
    kwargs:
      logging_interval : epoch

  RichProgressBar:
    is_use : true
    kwargs:
      refresh_rate_per_second: 0.1

  RichModelSummary:
    is_use: true
    kwargs:
      max_depth: 4

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 100
  detect_anomaly: false # makes it slower if true
  enable_progress_bar: True
  gradient_clip_val: 2 # null to disable
  gradient_clip_algorithm: "value" # "norm" to clip by norm (problem is if one val is inf, then all is 0)
  reload_dataloaders_every_n_epochs: 0
  log_every_n_steps: 200
  flush_logs_every_n_steps: 200
  val_check_interval: 1.0 # decrease for subepoch checkpointing
  enable_checkpointing: True

  # ENGINEERING / SPEED #
  gpus: 1
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed # TODO use "b16" once it is not experiemntal (maybe lightning 1.6)
  accumulate_grad_batches: 1
  sync_batchnorm: false # whether to synchronize batch norm over GPUs

  # DEBUGGING #
  fast_dev_run: false # use true to make a quick test (not full epoch)
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  profiler: null # use `simple` or `"advanced"` to find bottleneck

########## REPRESENTOR ##########
### DATA ###
data_repr:
  kwargs:
    dataset_kwargs:
      aux_target: ???

### MODELS ###
encoder:
  name: ???
  z_shape: 1024
  is_normalize_Z: False
  is_batchnorm_Z: False # drop if not useful
  batchnorm_mode: null # drop if not useful
  is_relu_Z: False # drop if not useful
  kwargs:
    in_shape: ${data.shape}
    out_shape: ${encoder.z_shape} # do not change that
    architecture: ???
    arch_kwargs: {}
    fam_kwargs: {}
    family: deterministic

online_evaluator:
  name: ???
  kwargs:
    in_shape: ${encoder.z_shape}
    out_shape: ${data.target_shape}
    architecture: ???
    arch_kwargs: {}
    is_classification: ${data.target_is_clf}

### OPTIMIZER ###
optimizer_issl:
  name: ${optimizer_issl.mode}_lr${format:${optimizer_issl.kwargs.lr},.1e}_w${format:${optimizer_issl.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_issl: {}

optimizer_eval:
  name: ${optimizer_eval.mode}_lr${format:${optimizer_eval.kwargs.lr},.1e}_w${format:${optimizer_eval.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_eval: {}

### RUNNING ###
checkpoint_repr: {}
# dictionary that will update the trainer (this is done because often the trainer is the same for repr and pred so want to minimize replication)
update_trainer_repr: {}

########## DOWNSTREAM TASK ##########
downstream_task:
  all_tasks: [pytorch_datarepr]


### DATA ###
# everything you want to override would go here
data_pred: {}

# all the defaults arguments go here.
dflt_data_pred:
  is_copy_repr: False
  kwargs:
    dataset_kwargs:
      aux_target: null

### PREDICTOR ###
predictor: {}

dflt_predictor:
  is_train: True

### OPTIMIZER ###
optimizer_pred:
  name: ${optimizer_pred.mode}_lr${format:${optimizer_pred.kwargs.lr},.1e}_w${format:${optimizer_pred.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_pred: {}

### RUNNING ###
checkpoint_pred: {}
# dictionary that will update the trainer (this is done because often the trainer is the same for repr and pred so want to minimize replication)
update_trainer_pred: {}

########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}