defaults:

  # GENERAL #
  - _self_
  - user

  - logger: wandb
  - server: none
  - hypopt: none # for hyperparameter tuning
  - mode: none

  # PRETRAINING #
  - data@data_repr: cifar10
  - architecture@encoder: resnet18
  - checkpoint@checkpoint_repr: last
  - optimizer@optimizer_issl: Adam_lr3e-3_w0
  - scheduler@scheduler_issl: cosine
  - decodability: contrastive
  - representor: base # actual model used for SSL

  # ONLINE EVALUATOR #
  # probing head trained during SSL
  - architecture@online_evaluator: linear
  - optimizer@optimizer_eval: Adam_lr3e-4_w0
  - scheduler@scheduler_eval: unifmultistep100 # ensure that can always keep up with the changing representation

  # DOWNSTREAM TASK #
  - checkpoint@checkpoint_pred: last # by default no validation set for predictors
  - optimizer@dflt_optimizer_pred: SGD # can be modified in downstream task
  - scheduler@dflt_scheduler_pred: cosine_nowarm # can be modified in downstream task

  # OVERRIDES #
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


########## GENERAL ##########
experiment: ???
job_num: ${hydra:job.num}
job_id: ??? # unique identifier
is_nlp_cluster: False # should the working directory be used as main
seed: 123
timeout: 1440 # 24 hours
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because time is not when job runs
# TODO should add epoch and batchsize in repr  (not doing for now for backward compatibility)
long_name_repr: exp_${experiment}/datarepr_${data_repr.name}/augrepr_${list2str:${data_repr.kwargs.dataset_kwargs.a_augmentations}}/repr_${representor.name}/dec_${decodability.name}/enc_${encoder.name}/reg_${regularizer.name}/optrepr_${optimizer_issl.name}/schedrepr_${scheduler_issl.name}/zdim_${encoder.z_shape}/zs_1/beta_${format:${representor.loss.beta},.1e}/seed_${seed}/addrepr_${other.add_repr}
long_name_pred: ${long_name_repr}/datapred_${data_pred.name}/pred_${predictor.name}/optpred_${optimizer_pred.name}/schedpred_${scheduler_pred.name}/eppred_${trainer.max_epochs}/bspred_${data_pred.kwargs.batch_size}/addpred_${other.add_pred}

paths: #! the best practice is not to modify those paths but to symlink them to the places you want
  relative_work: outputs/${now:%Y-%m-%d_%H-%M-%S}
  relative_checkpoint: checkpoints/${long_name}/jid_${job_id}
  work: ${hydra.runtime.cwd}/${paths.relative_work} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  base_dir: ${hydra:runtime.cwd} # actual script where you are running from and want to save stuff
  tmp_dir: ${paths.base_dir} # main directory for all things that are only used when running script
  wandb_dir: ${paths.tmp_dir}/wandb # main directory for all things that are only used when running script

  data: ${paths.base_dir}/data
  logs: ${paths.tmp_dir}/logs/${long_name}/jid_${job_id}
  checkpoint: ${paths.base_dir}/${paths.relative_checkpoint} # checkpoint to use during training
  exit_checkpoint: ${paths.base_dir}/${paths.relative_checkpoint} # checkpoint to use in case you get preempted

  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  pretrained:
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/**  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest

other: # some meta information that can be useful for internal stuff (usually dirty workarounds or for logging)
  add_repr: null # some additional value for saving (e.g. current sweeping values)
  add_pred: null # some additional value for saving (e.g. current sweeping values)

### STAGE SPECIFIC ###
stage: ???
long_name: ???
task: ??? # what to use for monitor in wandb. data.name during rep, task during pred
checkpoint: {}
data: {}

### RUNNING ###
evaluation:
  is_eval_on_test: True # whether to evaluate on test. If not uses validation which is necessary if you don't have access to test set
  representor:
    ckpt_path: "best"
    is_evaluate: ${representor.is_train}
    is_online_eval: True
  predictor:
    ckpt_path: "best"
    is_evaluate: ${predictor.is_train}

callbacks: # can use any callback name of issl.callbacks, pl.callbacks

  # all callback kwargs should be here (`is_use` just says whether to use that callback)
  LearningRateMonitor:
    is_use : true
    kwargs:
      logging_interval : epoch

  TQDMProgressBar: # TODO RichProgressBar in lightning 1.6
    is_use : ${trainer.enable_progress_bar }
    kwargs:
      refresh_rate: ${trainer.log_every_n_steps}

  RichModelSummary:
    is_use: true
    kwargs:
      max_depth: 4

regularizer:
  name: none # not used (backward compatibility for loading models)

trainer:
  #default_root_dir: ${paths.results}
  max_epochs: 100
  enable_progress_bar: True
  gradient_clip_val: 2 # null to disable
  gradient_clip_algorithm: "value" # "norm" to clip by norm (problem is if one val is inf, then all is 0)
  log_every_n_steps: 30
  val_check_interval: 1.0 # less than 1 for subepoch validation.
  check_val_every_n_epoch: 10
  enable_checkpointing: True

  # ENGINEERING / SPEED #
  gpus: 1
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed # TODO use "b16" once it is not experiemntal (maybe lightning 1.6)
  accumulate_grad_batches: 1
  sync_batchnorm: false # whether to synchronize batch norm over GPUs

  # DEBUGGING #
  fast_dev_run: false # use true to make a quick test (not full epoch)
  overfit_batches: 0.0 # use 0.01 to make sure you can overfit 1% of training data => training works
  profiler: null # use `simple` or `"advanced"` to find bottleneck

########## REPRESENTOR ##########
### DATA ###
data_repr:
  kwargs:
    dataset_kwargs:
      aux_target: ???

### MODELS ###
encoder:
  name: ???
  z_shape: 1024
  rm_out_chan_aug: False
  aux_enc_base: null
  is_etf_rep: False
  kwargs:
    in_shape: ${data.shape}
    out_shape: ${encoder.z_shape} # do not change that
    architecture: ???
    arch_kwargs: {}

online_evaluator:
  name: ???
  kwargs:
    in_shape: ${encoder.z_shape}
    out_shape: ${data.target_shape}
    architecture: ???
    arch_kwargs: {}
    is_classification: ${data.target_is_clf}

### OPTIMIZER ###
optimizer_issl:
  name: ${optimizer_issl.mode}_lr${format:${optimizer_issl.kwargs.lr},.1e}_w${format:${optimizer_issl.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_issl: {}

optimizer_eval:
  name: ${optimizer_eval.mode}_lr${format:${optimizer_eval.kwargs.lr},.1e}_w${format:${optimizer_eval.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_eval: {}

### RUNNING ###
checkpoint_repr: {}
# dictionary that will update the trainer (this is done because often the trainer is the same for repr and pred so want to minimize replication)
update_trainer_repr:
  max_epochs: 100

########## DOWNSTREAM TASK ##########
downstream_task:
  all_tasks: [torchlogistic_datarepr]

### DATA ###
# everything you want to override would go here
data_pred: {}

# all the defaults arguments go here.
dflt_data_pred:
  is_copy_repr: False
  kwargs:
    batch_size: 512
    dataset_kwargs:
      aux_target: null

### PREDICTOR ###
predictor: {}

dflt_predictor:
  is_train: True
  is_force_retrain: False

### OPTIMIZER ###
optimizer_pred: {}

dflt_optimizer_pred:
  name: ${.mode}_lr${format:${.kwargs.lr},.1e}_w${format:${.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0

scheduler_pred: {}
dflt_scheduler_pred: {}

### RUNNING ###
checkpoint_pred: {}
# dictionary that will update the trainer (this is done because often the trainer is the same for repr and pred so want to minimize replication)
update_trainer_pred:
  max_epochs: 100 # maybe should use 500

########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO

  run:
    dir: ${paths.work}

  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}