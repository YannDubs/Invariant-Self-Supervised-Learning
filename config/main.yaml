defaults:
  # all the following defaults select one config file per config folders.
  # for details about configs see hydra: https://hydra.cc/docs/intro/

  # GENERAL #
  - _self_
  - user

  - logger: wandb
  - server: none
  - hypopt: none # for hyperparameter tuning
  - mode: none

  # PRETRAINING #
  - data@data_repr: cifar10
  - architecture@encoder: resnet18
  - checkpoint@checkpoint_repr: last
  - optimizer@optimizer_issl: Adam_lr3e-3_w0
  - scheduler@scheduler_issl: cosine
  - decodability: cissl
  - representor: base # actual model used for SSL

  # ONLINE EVALUATOR / PROBING HEAD #
  - architecture@online_evaluator: linear
  - optimizer@optimizer_eval: Adam_lr3e-4_w0
  - scheduler@scheduler_eval: unifmultistep100 # ensure that can always keep up with the changing representation

  # DOWNSTREAM TASK #
  - checkpoint@checkpoint_pred: last
  - optimizer@dflt_optimizer_pred: SGD # can be modified in downstream task
  - scheduler@dflt_scheduler_pred: cosine_nowarm # can be modified in downstream task

  # OVERRIDES #
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


########## GENERAL ##########
seed: 123 # for reproducibility

### SERVER SPECIFIC ###
job_num: ${hydra:job.num} # set by hydra
job_id: ??? # unique identifier set by
is_nlp_cluster: False # specific to stanford's cluster
time: ${hydra:job.num}_${now:%Y-%m-%d_%H-%M-%S} # add job num because time is not when job runs
timeout: 1440 # 24 hours

### PATHS ###
experiment: ??? # set to the desired experiment name. **SET AS DESIRED**
long_name_repr: exp_${experiment}/datarepr_${data_repr.name}/augrepr_${list2str:${data_repr.kwargs.dataset_kwargs.a_augmentations}}/repr_${representor.name}/dec_${decodability.name}/enc_${encoder.name}/optrepr_${optimizer_issl.name}/schedrepr_${scheduler_issl.name}/zdim_${encoder.z_dim}/bs_${data_repr.kwargs.batch_size}/ep_${update_trainer_repr.max_epochs}/beta_1.0e-01/seed_${seed}/addrepr_${other.add_repr} # pretraining path
long_name_pred: ${long_name_repr}/datapred_${data_pred.name}/pred_${predictor.name}/optpred_${optimizer_pred.name}/schedpred_${scheduler_pred.name}/eppred_${trainer.max_epochs}/bspred_${data_pred.kwargs.batch_size}/addpred_${other.add_pred} # downstream task path
paths: #! the best practice is not to modify those paths but to symlink them to the places you want
  relative_work: outputs/${now:%Y-%m-%d_%H-%M-%S}
  relative_checkpoint: checkpoints/${long_name}/jid_${job_id}
  work: ${hydra.runtime.cwd}/${paths.relative_work} # unfortunately cannot use hydra: in hydra so need to do everything by hand i.e. cannot use ${paths.base_dir}/outputs/{time}
  base_dir: ${hydra:runtime.cwd} # actual script where you are running from and want to save stuff
  tmp_dir: ${paths.base_dir} # main directory for all things that are only used when running script
  wandb_dir: ${paths.tmp_dir}/wandb # main directory for all things that are only used when running script

  data: ${paths.base_dir}/data
  logs: ${paths.tmp_dir}/logs/${long_name}/jid_${job_id}
  checkpoint: ${paths.base_dir}/${paths.relative_checkpoint} # checkpoint to use during training
  exit_checkpoint: ${paths.base_dir}/${paths.relative_checkpoint} # checkpoint to use in case you get preempted

  results: ${paths.base_dir}/results/${long_name}/jid_${job_id}
  pretrained:
    save: ${paths.base_dir}/pretrained/${long_name}/jid_${job_id} # directory for saving pretrained models
    load: ${paths.base_dir}/pretrained/${long_name}/**  # directory for loading pretrained models if you use ** or * it will glob all matching files and take the latest

other: # some meta information that can be useful for internal stuff
  add_repr: null # some additional value for saving (e.g. current sweeping values)
  add_pred: null # some additional value for saving (e.g. current sweeping values)

### RUNNING ###
evaluation:
  is_eval_on_test: True # whether to evaluate on test rather than validation
  representor:
    ckpt_path: "best" # which checkpoint to evaluate
    is_evaluate: ${representor.is_train} # whether to evaluate representation learning stage
    is_online_eval: True # whether to train an online evaluator during SSL
  predictor:
    ckpt_path: "best" # which checkpoint to evaluate
    is_evaluate: ${predictor.is_train}  # whether to evaluate downstream predictor

callbacks: # can use any callback name of issl.callbacks, pl.callbacks
  LearningRateMonitor:
    is_use : true
    kwargs:
      logging_interval : epoch

  RichProgressBar:
    is_use : ${trainer.enable_progress_bar }
    kwargs:
      refresh_rate: ${trainer.log_every_n_steps}

  RichModelSummary:
    is_use: true
    kwargs:
      max_depth: 4

trainer: # this can be any arguments to a pytorch lightning trainer https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api
  # **SET AS DESIRED** any of the following arguments.
  # this will set both the trainer for the representor and the predictor.
  # To set parameters separately use `update_trainer_repr` and `update_trainer_pred`
  max_epochs: 100 # number of training epochs
  enable_progress_bar: True # whether to show progress bar
  gradient_clip_val: 2 # whether to clip gradients. use null to disable
  gradient_clip_algorithm: "value" # how to clip gradients. "norm" to clip by norm
  log_every_n_steps: 50 # how often to log
  check_val_every_n_epoch: 10 # how often to check validation
  val_check_interval: 1.0 # required to use check_val_every_n_epoch

  # ENGINEERING / SPEED #
  gpus: 1 # number of GP
  num_nodes: 1  # number gpu nodes
  precision: 16 # use 16 bit for speed # TODO use "b16" once it is not experiemntal (maybe lightning 1.6)

### STAGE SPECIFIC ###
# the following is modified at runtime, they do not need to be defined here
stage: ???
long_name: ???
task: ??? # what to use for monitor in wandb. data.name during rep, task during pred
checkpoint: {}
data: {}

########## PRETRAINING / REPRESENTATION LEARNING ##########
### DATA ###
# modified in the data config files
data_repr:
  kwargs:
    dataset_kwargs:
      aux_target: "augmentation" # input to teacher should be augmented example
      a_augmentations: ["data-standard"] # augmentations for teacher (default: uses standard)
      train_x_augmentations: "a_augmentations" # augmentations for student (default: same as teacher)

### MODELS ###
encoder: # modified in the encoder config files
  name: ???
  z_dim: 1024 # dimensionality of the representation. **SET AS DESIRED**
  kwargs:
    in_shape: ${data.shape}
    out_shape: ${encoder.z_dim} # do not change this (automatically set)
    architecture: ???
    arch_kwargs: {}

online_evaluator: # online probing head. modified in the architecture config files
  name: ???
  kwargs:
    in_shape: ${encoder.z_dim}
    out_shape: ${data.target_shape}
    architecture: ???
    arch_kwargs: {}

### OPTIMIZER ###
optimizer_issl: # optimizer for representation learning. modified in the optimizer config files
  name: ${optimizer_issl.mode}_lr${format:${optimizer_issl.kwargs.lr},.1e}_w${format:${optimizer_issl.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_issl: {}

optimizer_eval: # optimizer for online probe. modified in the optimizer config files
  name: ${optimizer_eval.mode}_lr${format:${optimizer_eval.kwargs.lr},.1e}_w${format:${optimizer_eval.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0
scheduler_eval: {}

### RUNNING ###
checkpoint_repr: {} # automatically set by checkpoint@checkpoint_repr
update_trainer_repr: # dictionary that will update the trainer configs only for the pretraining stage
  max_epochs: 100

########## DOWNSTREAM TASK ##########
# downstream tasks are defined by both the probe architecture/optimizer and the dataset

downstream_task:
  # the following is a list all tasks to evaluate the representations learning with
  # this will then use the appropriate config file in `downstream_task`
  all_tasks: [torchlogisticw1e-6_datarepr]

### DATA ###
# modified in the data config files (set by `all_tasks`)
data_pred: {}
dflt_data_pred: # all the default arguments
  is_copy_repr: False # whether to copy the data from the representation learning stage
  kwargs:
    batch_size: 512
    dataset_kwargs:
      aux_target: null

### PREDICTOR ###
# modified in the predictor config files (set by `all_tasks`)
predictor: {}
dflt_predictor:
  is_train: True
  is_force_retrain: False

### OPTIMIZER ###
# modified in the optimizer config files (set by `all_tasks`)
optimizer_pred: {}
dflt_optimizer_pred:
  name: ${.mode}_lr${format:${.kwargs.lr},.1e}_w${format:${.kwargs.weight_decay},.1e}
  mode: ???
  kwargs:
    lr: ???
    weight_decay: 0

# modified in the scheduler config files (set by `all_tasks`)
scheduler_pred: {}
dflt_scheduler_pred: {}

### RUNNING ###
checkpoint_pred: {} # automatically set by checkpoint@checkpoint_pred
update_trainer_pred: # dictionary that will update the trainer configs only for the downstream task stage
  max_epochs: 100

########## HYDRA ##########
hydra:
  job:
    env_set:
      NCCL_DEBUG: INFO
  run:
    dir: ${paths.work}
  sweep:
    dir:  ${paths.work}
    subdir: ${hydra.job.num}_${hydra.job.id}