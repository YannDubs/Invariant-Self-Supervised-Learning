# @package _global_
defaults:
  - base@representor
  - override /regularizer: none
  - override /decodability: distillating

representor:
  name: dstl_noema_mlp

data_repr:
  kwargs:
    dataset_kwargs:
      aux_target:  "augmentation"
      a_augmentations: ["data-standard"]
      train_x_augmentations: "a_augmentations"

decodability:
  kwargs:
    beta_pM_unif: 2.3
    beta_HMlZ: 1.8
    out_dim: 16384
    ema_weight_prior: null
    projector_kwargs:
      architecture: "mlp"
      hid_dim: 2048
      n_hid_layers: 2
      bottleneck_size: ${decodability.kwargs.predictor_kwargs.bottleneck_size}
      is_cosine: True
    predictor_kwargs:
      architecture: "mlp"
      hid_dim: 2048
      n_hid_layers: 2
      bottleneck_size: 512 # bottleneck for the last layer, ie, low rank approx