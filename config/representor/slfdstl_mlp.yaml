# @package _global_
defaults:
  - base@representor
  - override /regularizer: none
  - override /decodability: prior_self_distillation

representor:
  name: slfdstl

data_repr:
  kwargs:
    dataset_kwargs:
      aux_target:  "augmentation"
      a_augmentations: ["data-standard"]
      train_x_augmentations: "a_augmentations"

decodability:
  kwargs:
    mode: prior_self_distillation
    projector_kwargs:
      architecture: "mlp"
      hid_dim: 1024 # use same width as in `torch_mlp`
      bottleneck_size: null # bottleneck for the last layer, ie, low rank approx

encoder:
  is_relu_Z: True