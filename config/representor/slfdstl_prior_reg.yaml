# @package _global_
defaults:
  - base@representor
  - override /regularizer: huber
  - override /decodability: prior_self_distillation

representor:
  name: slfdstl_prior_reg

data_repr:
  kwargs:
    dataset_kwargs:
      aux_target:  "augmentation"
      a_augmentations: ["data-standard"]
      train_x_augmentations: "a_augmentations"

decodability:
  kwargs:
    is_ema: False
    is_process_Mx: False
    is_stop_grad:  False
    is_pred_proj_same:  False
    beta_pM_unif: 1.0 # TODO: tune
    ema_weight_prior: null # for MNIST 09 works best but likely for harder task needs much smaller
    predictor_kwargs:
      architecture: "linear"
      out_shape: 128
    projector_kwargs:
      architecture: "mlp"
      hid_dim: 2048
      n_hid_layers: 2
      norm_layer: "batch"
      out_shape: ${decodability.kwargs.predictor_kwargs.out_shape}

encoder:
  z_shape: 1024
  is_normalize_Z: False

