# @package _global_
defaults:
  - override /hydra/launcher: submitit_slurm

job_id: ${hydra:job.num}_${hydra:job.id} # job num is number in sweep. ID is unique ID like SLURM_JOB_ID

hydra:
  launcher:
    # maximum time for the job in minutes
    timeout_min: ${timeout}
    # number of cpus to use for each task
    cpus_per_task: 8
    # number of gpus to use on each node
    gpus_per_node: ${trainer.gpus}
    # number of tasks to spawn on each node
    tasks_per_node: 1 # number of tasks on single machine
    # memory to reserve for the job on each node (in GB)
    mem_gb: 32
    # number of nodes to use for the job
    nodes: ${trainer.num_nodes}
    # name of the job
    name: ${experiment}

    # slurm partition to use on the cluster
    partition: atlas
    comment: null
    constraint: null
    exclude: atlas5,atlas6,atlas8,atlas9
    array_parallelism: 30

    max_num_timeout: 3 # allow resume from checkpointing
    additional_parameters:
      gres: gpu:${trainer.gpus}
      #job_name: ${experiment} # to test
      #qos: normal # normal, high, deadline, nopreemption (this is the stream of execution )

# NOTE:
# I also had to change the command in submitit. specifically in submitit/slurm/slurm
# somewhere : /h/yannd/.conda/envs/lossyless/lib/python3.8/site-packages/submitit/slurm/
# function: _make_sbatch_string replace the end with
# lines += [
#       "",
#       "# command",
#       "export SUBMITIT_EXECUTOR=slurm",
#       "export MKL_THREADING_LAYER=GNU",
#       f"srun --mem {mem} --output '{stdout}' --error '{stderr}' --unbuffered {command}",
#   ]
#
# and also had to add slurm to the path
# export PATH="$PATH:/opt/slurm/bin"