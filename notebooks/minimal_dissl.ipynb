{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supreme-league",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning==1.7.2 in /usr/local/lib/python3.8/dist-packages (1.7.2)\n",
      "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
      "Requirement already satisfied: lightning-bolts==0.5 in /usr/local/lib/python3.8/dist-packages (0.5.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.8/dist-packages (2.10.0)\n",
      "\u001B[33mWARNING: You are using pip version 21.0.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3.8 -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning==1.7.2 scikit-learn==1.0.2 --no-dependencies tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-repeat",
   "metadata": {},
   "source": [
    "Basic variables depending on whether using GPU or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "compressed-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data_dir = \"data/\"\n",
    "if torch.cuda.is_available():\n",
    "    device, precision, gpus = \"cuda\", 16, 1\n",
    "else:\n",
    "    device, precision, gpus = \"cpu\", 32, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-litigation",
   "metadata": {},
   "source": [
    "## DISSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ahead-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "class DISSL(pl.LightningModule):\n",
    "    \"\"\"DISSL objective.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 n_equivalence_classes=16384, \n",
    "                 lambda_maximality=2.3, \n",
    "                 beta_det_inv=0.8, \n",
    "                 max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # ENCODER\n",
    "        self.encoder = resnet18()\n",
    "        self.encoder.fc = nn.Identity() # remove last linear layer\n",
    "        z_dim=512\n",
    "        \n",
    "        # TEACHER PROJECTION HEAD\n",
    "        # more expressive is better => MLP\n",
    "        n_hidden = 2048\n",
    "        bottleneck_size = 512 # adds a bottleneck to avoid linear layer with many parameters\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(z_dim, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_hidden, bottleneck_size),\n",
    "            nn.BatchNorm1d(bottleneck_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(bottleneck_size, self.hparams.n_equivalence_classes)\n",
    "        )\n",
    "        \n",
    "        # STUDENT PROJECTION HEAD: \n",
    "        # needs to be linear (could also add batchnorm as this is linear)\n",
    "        self.predictor = nn.Linear(z_dim, self.hparams.n_equivalence_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x1, x2 = batch\n",
    "        z1 = self.encoder(x1)\n",
    "        z2 = self.encoder(x2)\n",
    "        return (self.asymmetric_loss(z1,z2) + \n",
    "                self.asymmetric_loss(z2,z1)) / 2\n",
    "    \n",
    "    def asymmetric_loss(self, z1, z2, temperature_teach=0.5):\n",
    "        \"\"\"Computes the asymmetric DISSL loss where MC expectations over z1.\"\"\"\n",
    "        logits_t1 = self.projector(z1).float() / temperature_teach\n",
    "        logits_t2 = self.projector(z2).float() / temperature_teach\n",
    "        logits_s = self.predictor(z2).float() \n",
    "        \n",
    "        # q(\\hat{M}|X). batch shape: [batch_size] ; event shape: []\n",
    "        q_Mlx = Categorical(logits=logits_t1)\n",
    "        \n",
    "        # MAXIMALITY. -H[\\hat{M}]\n",
    "        q_M = Categorical(probs=q_Mlx.probs.mean(0))\n",
    "        mxml = -q_M.entropy() # you want to max entropy\n",
    "        \n",
    "        # INVARIANCE and DETERMINISM. E_{q(M|X)}[log q(M|\\tilde{X})]\n",
    "        det_inv = (q_Mlx.probs * logits_t2.log_softmax(-1)).sum(-1).mean()\n",
    "        \n",
    "        # DISTILLATION. E_{q(M|X)}[log s(M|\\tilde{X})]\n",
    "        dstl = (q_Mlx.probs * logits_s.log_softmax(-1)).sum(-1).mean()\n",
    "        \n",
    "        self.log_dict({\"H[M]\": -mxml, \"CE\": -det_inv}, prog_bar=True) \n",
    "\n",
    "        return self.hparams.lambda_maximality * mxml - self.hparams.beta_det_inv * det_inv - dstl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self(batch) \n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        return self.encoder(x).cpu().numpy(), y.cpu().numpy()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=2e-3, weight_decay=1e-6)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-registrar",
   "metadata": {},
   "source": [
    "## Data\n",
    "Downloads and prepare the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessory-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "pretrain_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=96, scale=(0.2, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grateful-province",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class PretrainSTL10(STL10):\n",
    "    def __init__(self, data_dir, pretrain_transforms, download=True):\n",
    "        super().__init__(data_dir, download=download, split=\"unlabeled\", transform=None)\n",
    "        self.pretrain_transforms = pretrain_transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x,y = super().__getitem__(index)\n",
    "        x1 = self.pretrain_transforms(x)\n",
    "        x2 = self.pretrain_transforms(x)\n",
    "        return x1, x2\n",
    "\n",
    "data_pretrain = PretrainSTL10(data_dir, pretrain_transforms=pretrain_transforms, download=True)\n",
    "data_train = STL10(data_dir,  split=\"train\", transform=val_transforms, download=True)\n",
    "data_test = STL10(data_dir, split=\"test\", transform=val_transforms, download=True)\n",
    "\n",
    "loader_pretrain = DataLoader(data_pretrain, batch_size=256, shuffle=True,\n",
    "                               num_workers=8, pin_memory=True)\n",
    "loader_train = DataLoader(data_train, batch_size=512, shuffle=False, num_workers=8, pin_memory=True, drop_last=False)\n",
    "loader_test = DataLoader(data_train, batch_size=512, shuffle=False, num_workers=8, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-latest",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "associate-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | encoder   | ResNet     | 11.2 M\n",
      "1 | projector | Sequential | 10.5 M\n",
      "2 | predictor | Linear     | 8.4 M \n",
      "-----------------------------------------\n",
      "30.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.1 M    Total params\n",
      "60.183    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93b26f22c634b27a26b776e415958df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100 \n",
    "dissl= DISSL(max_epochs=EPOCHS)\n",
    "trainer = pl.Trainer(gpus=gpus, precision=precision, max_epochs=EPOCHS, logger=False, enable_checkpointing=False)\n",
    "trainer.fit(dissl, train_dataloaders=loader_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-rendering",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "national-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.datamodules import SklearnDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coordinated-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57fe03f37a64ffda510de0f0ec037af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 391it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6273ec638a3c42b8bfab49b90e91d0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 391it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Z_train, Y_train = zip(*trainer.predict(dataloaders=loader_train, model=dissl))\n",
    "Z_test, Y_test = zip(*trainer.predict(dataloaders=loader_test, model=dissl))\n",
    "\n",
    "Z_train = np.concatenate(Z_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "Z_test = np.concatenate(Z_test, axis=0)\n",
    "Y_test = np.concatenate(Y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gothic-rating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downstream STL10 accuracy: 95.44%\n"
     ]
    }
   ],
   "source": [
    "# Downstream evaluation. Accuracy: 95.44% \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "best_acc = 0\n",
    "for C in np.logspace(-3,0,base=10,num=7):\n",
    "    clf = LinearSVC(C=C, dual=False)\n",
    "    clf.fit(Z_train, Y_train)\n",
    "    acc = clf.score(Z_test, Y_test)\n",
    "    best_acc = max(best_acc, acc)\n",
    "print(f\"Downstream STL10 accuracy: {best_acc*100:.2f}%\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}